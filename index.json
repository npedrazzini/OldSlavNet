[
{
	"uri": "https://npedrazzini.github.io/OldSlavNet/docs/install/",
	"title": "Install OldSlavNet",
	"tags": [],
	"description": "",
	"content": "1. Requirements  Python\u0026gt;=3.10.0  OldSlavNet v2.1 has only been tested on Python 3.10.0. There is no guarantee that it will work properly on earlier versions. Please update your Python version to 3.10.0 to make sure OldSlavNet works as expected.\n2. Get OldSlavNet Clone OldSlavNet\u0026rsquo;s GitHub repository:\ngit clone https://github.com/npedrazzini/OldSlavNet 3. Configure it From the root directory run:\nmake This will update pip if needed, install the following dependnencies in new virtual environment called oldslavnet-venv:\n cmake==3.22.0 Cython==0.29.24 numpy==1.21.4 dyNET  "
},
{
	"uri": "https://npedrazzini.github.io/OldSlavNet/docs/",
	"title": "Docs",
	"tags": [],
	"description": "",
	"content": "Full documentation on how to install and use OldSlavNet, as well as on how to train your own model. Click on any of the subsections in the sidebar to get started.\n"
},
{
	"uri": "https://npedrazzini.github.io/OldSlavNet/docs/preprocess/",
	"title": "Data preprocessing",
	"tags": [],
	"description": "",
	"content": "Input format OldSlavNet requires the Universal Dependencies 10-column CoNLL-U format as input. A ready-to-be-tagged CoNLL-U file may look like the following:\n1\tĞ´Ğ°\t_\t_\t_\t_\t_\t_\t_\t_ 2\tÑ‚Ñ¹Ñ‚ÑŠ\t_\t_\t_\t_\t_\t_\t_\t_ 3\tĞµÑĞ¼Ğ¸\t_\t_\t_\t_\t_\t_\t_\t_ 4\tĞ¶Ğ¸Ğ»\t_\t_\t_\t_\t_\t_\t_\t_ 5\tĞ²\t_\t_\t_\t_\t_\t_\t_\t_ 6\tÑ‡ĞµĞ±Ğ¾ĞºĞ°Ñ€Ñ£\t_\t_\t_\t_\t_\t_\t_\t_ 7\tÑ•Òƒ\t_\t_\t_\t_\t_\t_\t_\t_ 8\tĞ¼ÑÑ†ÑŒ\t_\t_\t_\t_\t_\t_\t_\t_ 1\tĞ´Ğ°\t_\t_\t_\t_\t_\t_\t_\t_ 2\tĞ²\t_\t_\t_\t_\t_\t_\t_\t_ 3\tÑĞ°Ñ€Ñ£\t_\t_\t_\t_\t_\t_\t_\t_ 4\tĞ¶Ğ¸Ğ»\t_\t_\t_\t_\t_\t_\t_\t_ 5\tĞ¼ÑÑ†ÑŒ\t_\t_\t_\t_\t_\t_\t_\t_ 6\tĞ²\t_\t_\t_\t_\t_\t_\t_\t_ 7\tĞ¼Ğ°Ğ·Ğ´Ñ€Ğ°Ğ½ÑŒÑĞºĞ¾Ğ¸\t_\t_\t_\t_\t_\t_\t_\t_ 8\tĞ·ĞµĞ¼Ğ»Ğ¸\t_\t_\t_\t_\t_\t_\t_\t_ Since OldSlavNet will only add annotation to columns 4 (part of speech), 7 (parent node\u0026rsquo;s index) and 8 (dependency relation), the other columns may already be filled and will not be overwritten (e.g. if you have already added lemmatization and morphological analysis). Any comment preceding each sentence will be disregarded. A partially filled, ready-to-be-annoted input file may then also look like this:\n# source = Afanasij Nikitinâ€™s journey beyond three seas, 7 # text = Ğ²Ğ¾ Ğ¸Ğ½Ğ´Ñ£Ğ¸ÑĞºĞ¾Ğ¸ Ğ·ĞµĞ¼Ğ»Ğ¸ Ğ³Ğ¾ÑÑ‚Ğ¸ ÑÑ§ ÑÑ‚Ğ°Ğ²Ñ§Ñ‚ÑŒ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ´Ğ²Ğ¾Ñ€ÑŒĞµĞ¼ÑŒ # sent_id = 195775 1\tĞ²Ğ¾\tĞ²ÑŠ\t_\t_\t_\t_\t_\t_\t_ 2\tĞ¸Ğ½Ğ´Ñ£Ğ¸ÑĞºĞ¾Ğ¸\tĞ¸Ğ½Ğ´Ñ£Ğ¸ÑĞºÑ‹Ğ¸\t_\t_\tCase=Loc|Degree=Pos|Gender=Fem|Number=Sing|Strength=Weak\t_\t_\t_\t_ 3\tĞ·ĞµĞ¼Ğ»Ğ¸\tĞ·ĞµĞ¼Ğ»Ñ\t_\t_\tCase=Loc|Gender=Fem|Number=Sing\t_\t_\t_\t_ 4\tĞ³Ğ¾ÑÑ‚Ğ¸\tĞ³Ğ¾ÑÑ‚ÑŒ\t_\t_\tCase=Nom|Gender=Masc|Number=Plur\t_\t_\t_\t_ 5\tÑÑ§\tÑĞµĞ±Ğµ\t_\t_\tCase=Acc|Number=Sing|Person=3\t_\t_\t_\t_ 6\tÑÑ‚Ğ°Ğ²Ñ§Ñ‚ÑŒ\tÑÑ‚Ğ°Ğ²Ğ¸Ñ‚Ğ¸\t_\t_\tMood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t_\t_\t_\t_ 7\tĞ¿Ğ¾\tĞ¿Ğ¾\t_\t_\t_\t_\t_\t_\t_ 8\tĞ¿Ğ¾Ğ´Ğ²Ğ¾Ñ€ÑŒĞµĞ¼ÑŒ\tĞ¿Ğ¾Ğ´Ğ²Ğ¾Ñ€Ğ¸Ğµ\t_\t_\tCase=Dat|Gender=Neut|Number=Plur\t_\t_\t_\t_ # source = Afanasij Nikitinâ€™s journey beyond three seas, 7 # text = Ğ° Ñ£ÑÑ‚Ğ¸ Ğ²Ğ°Ñ€Ñ§Ñ‚ÑŒ Ğ½Ğ° Ğ³Ğ¾ÑÑ‚Ğ¸ Ğ³Ğ¾ÑĞ¿Ğ¾Ğ´Ğ°Ñ€Ñ‹Ğ½Ğ¸ Ğ¸ Ğ¿Ğ¾ÑÑ‚ĞµĞ»Ñ ÑÑ‚ĞµĞ»ÑÑ‚ÑŒ Ğ¸ ÑĞ¿Ñ§Ñ‚ÑŒ Ñ Ğ³Ğ¾ÑÑ‚ÑŒĞ¼Ğ¸ # sent_id = 195118 1\tĞ°\tĞ°\t_\t_\t_\t_\t_\t_\t_ 2\tÑ£ÑÑ‚Ğ¸\tÑ£ÑÑ‚Ğ¸\t_\t_\tTense=Pres|VerbForm=Inf|Voice=Act\t_\t_\t_\t_ 3\tĞ²Ğ°Ñ€Ñ§Ñ‚ÑŒ\tĞ²Ğ°Ñ€Ğ¸Ñ‚Ğ¸\t_\t_\tMood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t_\t_\t_\t_ 4\tĞ½Ğ°\tĞ½Ğ°\t_\t_\t_\t_\t_\t_\t_ 5\tĞ³Ğ¾ÑÑ‚Ğ¸\tĞ³Ğ¾ÑÑ‚ÑŒ\t_\t_\tCase=Acc|Gender=Masc|Number=Plur\t_\t_\t_\t_ 6\tĞ³Ğ¾ÑĞ¿Ğ¾Ğ´Ğ°Ñ€Ñ‹Ğ½Ğ¸\tĞ³Ğ¾ÑĞ¿Ğ¾Ğ´Ğ°Ñ€Ñ‹Ğ½Ğ¸\t_\t_\tCase=Nom|Gender=Fem|Number=Plur\t_\t_\t_\t_ 7\tĞ¸\tĞ¸\t_\t_\t_\t_\t_\t_\t_ 8\tĞ¿Ğ¾ÑÑ‚ĞµĞ»Ñ\tĞ¿Ğ¾ÑÑ‚ĞµĞ»Ñ\t_\t_\tCase=Acc|Gender=Fem|Number=Sing\t_\t_\t_\t_ 9\tÑÑ‚ĞµĞ»ÑÑ‚ÑŒ\tÑÑ‚ÑŒĞ»Ğ°Ñ‚Ğ¸\t_\t_\tMood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t3\t_\t_\t_ 10\tĞ¸\tĞ¸\t_\t_\t_\t_\t_\t_\t_ 11\tÑĞ¿Ñ§Ñ‚ÑŒ\tÑÑŠĞ¿Ğ°Ñ‚Ğ¸\t_\t_\tMood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t_\t_\t_\t_ 12\tÑ\tÑÑŠ\t_\t_\t_\t_\t_\t_\t_ 13\tĞ³Ğ¾ÑÑ‚ÑŒĞ¼Ğ¸\tĞ³Ğ¾ÑÑ‚ÑŒ\t_\t_\tCase=Ins|Gender=Masc|Number=Plur\t_\t_\t_\t_ Convert a text file to CoNLL-U Start from a tokenized and sentencized text file containing your to-be input file, looking like the following (i.e. one sentence per line, with tokens separated by white space, including any punctuation sign):\nĞ´Ğ° Ñ‚Ñ¹Ñ‚ÑŠ ĞµÑĞ¼Ğ¸ Ğ¶Ğ¸Ğ» Ğ² Ñ‡ĞµĞ±Ğ¾ĞºĞ°Ñ€Ñ£ Ñ•Òƒ Ğ¼ÑÑ†ÑŒ Ğ´Ğ° Ğ² ÑĞ°Ñ€Ñ£ Ğ¶Ğ¸Ğ» Ğ¼ÑÑ†ÑŒ Ğ² Ğ¼Ğ°Ğ·Ğ´Ñ€Ğ°Ğ½ÑŒÑĞºĞ¾Ğ¸ Ğ·ĞµĞ¼Ğ»Ğ¸ Ğ° Ñ¿Ñ‚Ñ¹Ğ´Ñ‹ ĞºĞ¾ Ğ°Ğ¼Ğ¸Ğ»Ğ¸ Ğ¸ Ñ‚Ñ¹Ñ‚ÑŠ Ğ¶Ğ¸Ğ»ÑŠ ĞµÑĞ¼Ğ¸ Ğ¼ÑÑ†ÑŒ Ğ° Ñ¿Ñ‚Ñ¹Ğ´Ñ‹ Ğº Ğ´Ğ¸Ğ¼Ğ¾Ğ²Ğ°Ğ½Ñ‚ê™‹ Ğ° Ğ¸Ğ· Ğ´Ğ¸Ğ¼Ğ¾Ğ²Ğ°Ğ½Ñ‚Ñ¹ ĞºĞ¾ Ñ€Ñ£Ñ Ğ° Ñ‚Ñ¹ ê™‹Ğ±Ğ¸Ğ»Ğ¸ ÑˆĞ°ê™‹ÑĞµĞ½Ñ§ Ğ°Ğ»ĞµĞµĞ²Ñ‹Ñ… Ğ´ĞµÑ‚ĞµĞ¸ Ğ¸ Ğ²Ğ½Ñ¹Ñ‡Ğ°Ñ‚ÑŠ Ğ¼Ğ°Ñ…Ğ¼ĞµÑ‚ĞµĞ²Ñ‹Ñ… . Ğ¸ Ñ¡Ğ½ÑŠ Ğ¸Ñ… Ğ¿Ñ€Ğ¾ĞºĞ»Ñ§Ğ»ÑŠ Ğ¸Ğ½Ğ¾ Ğ¾Òƒ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ¾Ğ²ÑŠ ÑÑ§ Ñ€Ğ¾Ğ·Ğ²Ğ°Ğ»Ğ¸Ğ»Ğ¾ Ğ° Ğ¸Ğ·Ğ´ Ñ€Ñ£Ñ§ Ğº ĞºĞ°ÑˆĞµĞ½Ğ¸ Ğ¸ Ñ‚Ñ¹Ñ‚ÑŠ ĞµÑĞ¼Ğ¸ Ğ±Ñ‹Ğ» Ğ¼ÑÑ†ÑŒ . Ğ° Ğ¸Ğ· ĞºĞ°ÑˆĞµĞ½Ğ¸ Ğº Ğ½Ğ°Ğ¸Ğ½Ñ¹ Ğ° Ğ¸Ğ· Ğ½Ğ°Ğ¸Ğ½Ğ° ĞºĞ¾ ĞµĞ·Ğ´Ñ£Ğ¸ Ğ¸ Ñ‚Ñ¹Ñ‚ÑŠ Ğ¶Ğ¸Ğ»ÑŠ ĞµÑĞ¼Ğ¸ Ğ¼ÑÑ†ÑŒ Ğ° Ğ¸Ğ· Ğ´Ğ¸ĞµÑÑŠ ĞºÑŠ ÑÑ‹Ñ€Ñ‡Ğ°Ğ½Ñ¹ Assuming such text file is called text1 (avoid adding .txt extension to the file name), run the following:\npython3 ./scripts/preprocess/converter.py path/to/text1 This will generate a text1.conllu output ready to be annotated by OldSlavNet.\n"
},
{
	"uri": "https://npedrazzini.github.io/OldSlavNet/data/",
	"title": "Data",
	"tags": [],
	"description": "",
	"content": "Training set: language varieties Below we report details of each text included as training data for OldSlavNet. This is to provide users who wish to replicate or improve our results with a transparent dataset breakdown.\nIf you are not satisfied with OldSlavNet\u0026rsquo;s parsing performance on your particular texts, you are very welcome to send us any suggestions (here) on how we might improve it, particularly on the basis of expert evaluation of the data themselves (e.g. distribution and representativeness of different pre-modern Slavic varieties and Church Slavonic recensions). Alternative, you may want to train your own model!\nThe following are all the texts used as training data, their language variety, and number of tokens. The proposed language classification is by no means definitive, since several texts present mixed regional features and are contained in much later manuscripts than the varieties they represent as a whole.\n   Variety Text Tokens     OCS Codex Marianus 58,269    Codex Suprasliensis 79,070    Codex Zographensis 1,098    Kiev Missal 370    Psalterium Sinaiticum 248   SCS Vita Constantini 890   RCS Vita Methodii 331   OES Primary Chronicle (Codex Laurentianus) 56,725    Suzdal Chronicle (Codex Laurentianus) 23,760    Primary Chronicle (Codex Hypathianus) 3,610    First Novgorod Chronicle (Synodal) 17,838    Kiev Chronicle (Codex Hypathianus) 544    Colophon (Mstislav\u0026rsquo;s Gospel) 259    Colophon (Ostromir Codex) 199    Missive (Archbishop of Riga) 171    Mstislav\u0026rsquo;s letter 158    Novgorodâ€™s treaty with Jaroslav 423    Russkaja pravda 4,174    Statute of Prince Vladimir 495    Treaty (Smolensk-Riga-Gotland) 1,421    The Tale of Igorâ€™s Campaign 2,850    Russkaja pravda 4,174    Uspenskij Sbornik (excerpts) 25,189    Varlaam Xutynskij\u0026rsquo;s Grant Charter 148   MRus Afanasij Nikitin\u0026rsquo;s \\textit{Journey} 6,842    Charter of Prince Jurij Svjatoslavich 344    Correspondence of Peter the Great 100    Domostroj 23,459    Life of Sergij of Radonezh 20,361    History of the schism (materials) 1,835    Missive (Ivan of Pskov) 339    Testament (Ivan Jur\u0026rsquo;eviÄ Graznoj) 421    Life of Avvakum 22,835    Tale of Dracula 2,487    The tale of Luka KoloÄskij 906    The taking of Pskov 2,326    The tale of the fall of Constantinople 9,258    Vesti-Kuranty 1,154    ZadonÅ¡Äina 2,399   ONov Birchbark letters 1,965    Novgorod service book marginalia 93    Novgorodians' losses 187   ModRus SynTagRus 18,355   BCS UD Serbian-SET 17,622    "
},
{
	"uri": "https://npedrazzini.github.io/OldSlavNet/docs/tag/",
	"title": "Annotate new texts",
	"tags": [],
	"description": "",
	"content": "Requirements All the texts to be annotated must be preprocessed as shown in the Data Preprocessing section and placed inside the folder ./test_data/tobeannotated/:\nğŸ“¦ROOT â”£ ğŸ“‚models â”£ ğŸ“‚oldslavnet-venv â”£ ğŸ“‚scripts â”£ ğŸ“‚test_data â”ƒ â”£ ğŸ“‚annotated â”ƒ â”£ ğŸ“‚gold â”ƒ â”— ğŸ“‚tobeannotated â”ƒ â”£ ğŸ“œtext1.conllu â”ƒ â”— ğŸ“œtext2.conllu â”£ ğŸ“‚training_data â”£ ğŸ“œLICENSE â”£ ğŸ“œMakefile â”£ ğŸ“œREADME.md â”£ ğŸ“œrequirements.txt â”£ ğŸ“œtag.sh â”— ğŸ“œtrain.sh Annotate your texts From the ROOT directory run:\n./tag.sh You will be prompted to enter:\n The name of the model you want to use to tag your texts (press Enter for \u0026lsquo;OldSlavNet\u0026rsquo;) The name of the text you want to annotate (press Enter for \u0026lsquo;everything inside the folder ./test_data/tobeannotated/').  Your tagged texts will be saved under ./test_data/annotated/ with the same name as the original text:\nğŸ“¦ROOT â”£ ğŸ“‚models â”£ ğŸ“‚oldslavnet-venv â”£ ğŸ“‚scripts â”£ ğŸ“‚test_data â”ƒ â”£ ğŸ“‚annotated â”ƒ â”ƒ â”£ ğŸ“œtext1.conllu â”ƒ â”ƒ â”— ğŸ“œtext2.conllu â”ƒ â”£ ğŸ“‚gold â”ƒ â”— ğŸ“‚tobeannotated â”ƒ â”£ ğŸ“œtext1.conllu â”ƒ â”— ğŸ“œtext2.conllu â”£ ğŸ“‚training_data â”£ ğŸ“œLICENSE â”£ ğŸ“œMakefile â”£ ğŸ“œREADME.md â”£ ğŸ“œrequirements.txt â”£ ğŸ“œtag.sh â”— ğŸ“œtrain.sh "
},
{
	"uri": "https://npedrazzini.github.io/OldSlavNet/contacts/",
	"title": "Contacts",
	"tags": [],
	"description": "",
	"content": "Contact us for collaborations, assistance, or suggestions for improvement:\nnilo.pedrazzini@ling-phil.ox.ac.uk or nilo.pedrazzini@gmail.com\n"
},
{
	"uri": "https://npedrazzini.github.io/OldSlavNet/docs/train/",
	"title": "Train a new model",
	"tags": [],
	"description": "",
	"content": "Requirements To train your own parser, you will need:\n train.conllu: the training data. Put this file inside the folder ./training_data/new/ dev.conllu: the development data. Put this file inside the folder ./training_data/new/ (Optional) validation and gold data: while the training script will still work without these data, it is of course good practice to test the parser\u0026rsquo;s performance on a small batch of unseen data representative of the kind of texts you may want to use the parser for. Put any number of \u0026lsquo;gold-standard\u0026rsquo; texts for which you wish to check the parser\u0026rsquo;s performance individually inside the folder ./test_data/gold/, and a copy of each of these inside ./test_data/tobeannotated/  After adding these files to the right directories, the project directory should look like the following (note the placement of train.conllu, dev.conllu, and of the gold-test data, named text1.conllu and text2.conllu):\nğŸ“¦ROOT â”£ ğŸ“‚models â”ƒ â”— ğŸ“‚OldSlavNet â”ƒ â”£ ğŸ“œmodel â”ƒ â”— ğŸ“œmodel.params â”£ ğŸ“‚oldslavnet-venv â”£ ğŸ“‚scripts â”£ ğŸ“‚test_data â”ƒ â”£ ğŸ“‚annotated â”ƒ â”£ ğŸ“‚gold â”ƒ â”ƒ â”£ ğŸ“œtext1.conllu â”ƒ â”ƒ â”— ğŸ“œtext2.conllu â”ƒ â”— ğŸ“‚tobeannotated â”ƒ â”£ ğŸ“œtext1.conllu â”ƒ â”— ğŸ“œtext2.conllu â”£ ğŸ“‚training_data â”ƒ â”£ ğŸ“‚new â”ƒ â”ƒ â”£ ğŸ“œdev.conllu â”ƒ â”ƒ â”— ğŸ“œtrain.conllu â”ƒ â”— ğŸ“‚past â”ƒ â”— ğŸ“‚OldSlavNet â”ƒ â”£ ğŸ“œdev.conllu â”ƒ â”— ğŸ“œtrain.conllu â”£ ğŸ“œLICENSE â”£ ğŸ“œMakefile â”£ ğŸ“œREADME.md â”£ ğŸ“œrequirements.txt â”£ ğŸ“œtag.sh â”— ğŸ“œtrain.sh Train the model From the ROOT directory, run:\n./train.sh You will be prompted to enter:\n A name for your new model (e.g. newmodel1) Your choice of hyperparameters (press Enter to keep default):  training epochs number of BiLSTM dimensions number of BiLSTM layers size of MLP hidden layer size of word embeddings size of character embeddings size of POS tag embeddings    This will:\n Create a folder under ./models/ named after the name you entered for your model, where the trained model itself (the model and model.params files) will be saved Move your training and development data from ./training_data/new/ to a new folder under ./training_data/past/ named after the name you entered for your model Train your model Annotate the texts in ./test_data/tobeannotated/, compare them with those with the same name under ./test_data/gold/ and generate a text file for each of them with performance metrics under ./models/yourmodelname/validation-output/  After the model has been trained, the project directory should look like the following:\nğŸ“¦ROOT â”£ ğŸ“‚models â”ƒ â”£ yourmodelname â”ƒ â”ƒ â”£ ğŸ“œmodel â”ƒ â”ƒ â”£ ğŸ“œmodel.params â”ƒ â”ƒ â”— ğŸ“‚validation-output â”ƒ â”ƒ â”£ ğŸ“œtext1-validated.txt â”ƒ â”ƒ â”— ğŸ“œtext2-validated.txt â”ƒ â”— ğŸ“‚OldSlavNet â”ƒ â”£ ğŸ“œmodel â”ƒ â”— ğŸ“œmodel.params â”£ ğŸ“‚oldslavnet-venv â”£ ğŸ“‚scripts â”£ ğŸ“‚test_data â”£ ğŸ“‚training_data â”ƒ â”£ ğŸ“‚new â”ƒ â”— ğŸ“‚past â”ƒ â”— ğŸ“‚yourmodelname â”ƒ â”£ ğŸ“œdev.conllu â”ƒ â”— ğŸ“œtrain.conllu â”ƒ â”— ğŸ“‚OldSlavNet â”ƒ â”£ ğŸ“œdev.conllu â”ƒ â”— ğŸ“œtrain.conllu â”£ ğŸ“œLICENSE â”£ ğŸ“œMakefile â”£ ğŸ“œREADME.md â”£ ğŸ“œrequirements.txt â”£ ğŸ“œtag.sh â”— ğŸ“œtrain.sh "
},
{
	"uri": "https://npedrazzini.github.io/OldSlavNet/",
	"title": "OldSlavNet",
	"tags": [],
	"description": "Pre-modern Slavic PoS tagger and dependency parser",
	"content": "OldSlavNet OldSlavNet is a generic pre-modern Slavic dependency parser, described in:\n Pedrazzini, Nilo \u0026amp; Hanne M. Eckhoff. 2021. OldSlavNet: A scalable Early Slavic dependency parser trained on modern language data. Software Impacts 100063.  @article{oldslavnet, title={OldSlavNet: A scalable Early Slavic dependency parser trained on modern language data}, author={Pedrazzini, Nilo and Eckhoff, Hanne M.}, journal={Software Impacts}, pages={100063}, year={2021}, } Version 3.0.0 Version 3.0.0 has simplified the parser\u0026rsquo;s configuration and usage.\n To install all the dependencies users only need to run a Makefile. To annotate a text or series of texts, they only need to add the pre-preocessed texts to a specific directory and run a single Shell script which by default will tag all the texts in that directory. A new Shell script has been added to train, give a name to and test your own model.  "
},
{
	"uri": "https://npedrazzini.github.io/OldSlavNet/_footer/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "(c) 2021 Nilo Pedrazzini, MIT Licence\n"
},
{
	"uri": "https://npedrazzini.github.io/OldSlavNet/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://npedrazzini.github.io/OldSlavNet/credits/",
	"title": "Credits",
	"tags": [],
	"description": "contributors and packages used by hugo-theme-docdock",
	"content": "Contributors  Nilo Pedrazzini Hanne M. Eckhoff  Hugo DocDock Theme https://docdock.netlify.app\n"
},
{
	"uri": "https://npedrazzini.github.io/OldSlavNet/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]